import torch
from .generate import get_model
from data.prepare_data import tokenizer
import os
import argparse

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def generate_speculative(main_model, draft_model, input_ids, max_new_tokens=512, device=DEVICE, gamma=5, use_cache=False, return_stats=False):
    '''
    Speculative generation function which will utilize draft model to speculate gamma tokens, then verify it with main model in one pass
    
    Args:
        main_model: The main model
        draft_model: The draft model
        input_ids: Input sequence of shape: (B, T)
        max_new_tokens: Maximum number of tokens to generate
        device: Device to use for generation
        gamma: Number of tokens to speculate using draft model
        use_cache: Boolean variable to determine whether to use cache or not
        return_stats: Boolean variable to determine whether to return generation stats or not
    '''
    
    # If model is using cache, then reset cache before generation
    if use_cache:
        for block in draft_model.blocks:
            block.sa_heads.reset_cache()
        
    # Pass the input ids once to prefill the decoder cache
    speculated_ids = draft_model.generate(
        input_ids,
        max_new_tokens=gamma,
        use_cache=use_cache
    )
    
    tokens_generated = 0
    total_accepted_tokens = 0
    draft_generated_tokens = 0
    total_steps = 0
    while tokens_generated < max_new_tokens:
        input_len = input_ids.shape[1]
        # Take the new tokens speculated by the draft model
        draft_tokens = speculated_ids[:, input_len:]
        
        # Pass the speculated ids to the main model for verification
        target_logits, _ = main_model(speculated_ids, use_cache=False)
        
        # Shift indices by -1 to align the target model's output logits with the draft tokens they are predicting.
        verification_logits = target_logits[:, input_len - 1 : input_len + gamma - 1, :]
        main_tokens = torch.argmax(verification_logits, dim=-1)
        
        # Calculate the total number of accepted tokens
        accepted_tokens = 0
        for i in range(gamma):
            draft_token = draft_tokens[0, i]
            main_token = main_tokens[0, i]
            
            if draft_token == main_token:
                accepted_tokens += 1
            else:
                break
        
        # Take the last correct token predicted by the main model
        correction_token = torch.argmax(target_logits[:, input_len + accepted_tokens - 1, :], dim=-1)
        # Valid draft is all the draft tokens which are accepted by the main model
        valid_draft = draft_tokens[:, :accepted_tokens]
        
        # Concatenate the valid draft and correction token to get the new input ids
        new_tokens = torch.cat([valid_draft, correction_token.unsqueeze(0)], dim=1)
        input_ids = torch.cat([input_ids, new_tokens], dim=1)
        
        # Increment the counters accordingly
        tokens_generated += (accepted_tokens + 1)
        total_accepted_tokens += accepted_tokens
        draft_generated_tokens += gamma
        total_steps += 1
        
        # If the draft model is using cache, rollback the KV Cache to the actual valid length
        if use_cache:
            valid_len = input_len + accepted_tokens
            for block in draft_model.blocks:
                block.sa_heads.truncate_cache(valid_len)
            
            draft_input = correction_token.unsqueeze(0)
        else:
            draft_input = input_ids
        
        # Speculate the next chunk using the draft model
        next_chunk = draft_model.generate(
            draft_input,
            max_new_tokens=gamma,
            use_cache=use_cache
        )
        
        # Generate the next speculated ids
        input_length_passed = draft_input.shape[1]
        newly_generated_gamma = next_chunk[:, input_length_passed:]
        speculated_ids = torch.cat([input_ids, newly_generated_gamma], dim=1)        
    
    # Acceptance rate is the total number of tokens accepted divided by total number of tokens generated by the draft model
    acceptance_rate = total_accepted_tokens / draft_generated_tokens
    # Mean accepted tokens is the average number of tokens accepted in each step
    mean_accepted = total_accepted_tokens / total_steps
    
    if return_stats:
        return input_ids, acceptance_rate, mean_accepted
    else:
        return input_ids
            
if __name__ == '__main__':
    # CLI Arguments
    parser = argparse.ArgumentParser("Speculative Inference Engine")
    
    parser.add_argument(
        "--draft_model", type=str, default="draft_medium",
        choices=["draft_small", "draft_medium"], help="Select draft model for speculative generation"
    )
    parser.add_argument(
        "--gamma", type=int, default=5, help="Number of draft tokens to speculate per step"
    )
    parser.add_argument(
        "--no_cache", action="store_true", help="Disable KV Cache"
    )
    parser.add_argument(
        "--return_stats", action="store_true", help="Return metrics"
    )
    args = parser.parse_args()
    
    draft_model_name = args.draft_model
    gamma = args.gamma
    use_cache = not args.no_cache
    return_stats = args.return_stats
    
    # Load both models
    print(">> Loading Main Model...")
    main_model = get_model(model_name="main")
    print("\n>> Loading Draft Model...")
    draft_model = get_model(model_name=draft_model_name)
    
    if main_model and draft_model:
        # Put both models in eval model
        main_model.eval()
        draft_model.eval()
        
        # Take the prompt as input and tokenize it
        prompt = input("\nPlease enter the prompt: ")
        input_ids = torch.tensor(
            [tokenizer.encode(prompt)],
            dtype=torch.long,
            device=DEVICE
        )
        
        if return_stats:
            output_ids, acceptance_rate, mean_accepted = generate_speculative(main_model, draft_model, input_ids, gamma=gamma, use_cache=use_cache, return_stats=return_stats)
        else:
            output_ids = generate_speculative(main_model, draft_model, input_ids, gamma=gamma, use_cache=use_cache, return_stats=return_stats)

        text = tokenizer.decode(output_ids[0].tolist())
        print(f"\n>> Output: {text}")
        if return_stats:
            print(f"\n>> Acceptance rate: {acceptance_rate}")
            print(f">> Mean Accepted tokens: {mean_accepted}")